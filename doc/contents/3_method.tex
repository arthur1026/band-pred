\section{Regression Models}
\label{sec:method}
As introduced before, regression~\cite{hastie2009elements} is the process to reveal the relationship between input vector and output value.
Mathematically, we denote the input vector as $X = (x_1, x_2, \ldots, x_p)$, and the real-value output $Y$.
Therefore, regression is the process to seek the best function $f(x)$ such that $Y = f(X)$.
Generally, we first make a reasonable hypothesis of the underlying $f(x)$ through observation of $X$ and corresponding $Y$.
With abundant observations $X$ and ground truth value $Y$, we then could estimate the hypothesis $f(x)$ by automatically tuning the parameters to minimize the difference between $f(X)$ and $Y$.
This is defined as supervised learning in machine learning literature, since we provide both input $X$ and ground truth output $Y$ to train the underlying model $f(x)$.
And the training process is essentially to estimate the best parameters of $f(x)$.

In the following sessions, we give brief introduction to regression models that have been used in our system.
Due to the limitation of pages, we provide detailed references for curious readers to deeply understand the mathematical meanings behind each model.

\subsection{Linear models}
\label{sub:linear_models}
As indicated from the name, linear models assume a linear relationship between input vector $X$ and real-value output $Y$ as follows~\cite{hastie2009elements}:
\begin{equation}
	Y = w_0 + \sum\limits_{i=1}^p w_i{x_i}
\end{equation}
where $w_i$ are coefficients and $w_0$ is the intercept. For simplicity reasons, we can add one more dimension to $X$ with value 1 so that we could treat intercept as coefficient as well:
\begin{equation}
	\label{equ:linear}
	\begin{aligned}
		Y &= Xw \\
		where \quad X &= (1, x_1, x_2, \ldots, x_p) \\
		w &= {(w_0, w_1, w_2, \ldots, w_p)}^T
	\end{aligned}
\end{equation}
We use the notation in Section~\ref{equ:linear} through all the rest of the text.

\subsubsection{Lasso}
\label{ssub:lasso}
LASSO~\cite{Tibshirani1996}, Least Absolute Shrinkage and Selection Operator, is a linear model that estimates sparse coefficients, which results from the additional $l_1$ constraint on coefficients.
It is useful in certain context due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent.

Mathematically, it consists of least square residual error with $l_1$ prior as regularizer.
The objective function to minimize is:
\begin{equation}
  \min_w\frac{1}{2N}\|Xw - y\|^2_2 + \alpha\|w\|_1
\end{equation}
where $N$ is the number of samples, and $\alpha$ is a constant to balance the weight between those two costs.

\subsubsection{Elastic net}
\label{ssub:elastic_net}
In addition to $l_1$ regularization on coefficients $w$ like in LASSO(Section~\ref{ssub:lasso}), the elastic net~\cite{zou2005} method imposes $l_2$ regularization as well.
The mathematical formulation is shown below:
\begin{equation}
  \min_w\frac{1}{2N}\|Xw - y\|^2_2 + \alpha\rho\|w\|_1 + \frac{1}{2}\alpha(1-\rho)\|w\|^2_2
\end{equation}

Overall, LASSO is a special case of elastic net with $\rho = 1$.
With $l_2$ regularization, elastic net is able to overcome some limitations of LASSO\@.
In general, if there is a group of highly correlated features, LASSO will just pick one of the features and ignore the others.
The elastic net, conversely, will tend to use all these features, which is desirable from a generalization point of view.

\subsection{Ensemble methods}
\label{sub:ensemble_methods}
Ensemble methods are to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalization ability/ robustness over a single estimator.
There are two distinguishable families of ensemble methods:
\begin{itemize}
	\item \textbf{Average methods}: the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced. Random forest in Section~\ref{ssub:random_forests} belongs to this category.
	\item \textbf{Boosting methods}: base estimators are built sequentially and one tried to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble. We review two classical methods belong to this category: AdaBoost in Section~\ref{ssub:adaboost} and Gradient Boost in Section~\ref{ssub:gradient_boost}.
\end{itemize}

\subsection{Regression Trees}
\label{ssub:regression_tree}
We first give a brief introduction to regression trees~\cite{hastie2009elements}, which form the basis for the entire ensemble methods.
Similar to decision trees, regression trees partition the feature space into a set of rectangles and then fit a simple model (like a constant) in each one.
Unlike decision trees which output is a discrete label, regression trees are used to predict real-value output.
Suppose a general regression tree has a partition into $M$ regions $R_1, R_2, \ldots, R_M$, and we model the response as a constant $c_m$ in each region:
\begin{equation}
	f(x) = \sum \limits_{m=1}^{M} (c_m)I(x \in R_m)
\end{equation}
And for each region $R_m$, the constant $c_m$ is just the average of the $y_i$ in the region:
\begin{equation}
	c_m  = ave(y_i | x_i \in R_m)
\end{equation}

One of the advantage of regression trees is the flexibility in control of over-fitting.
This is done by restrict the size of the tree: Clearly a very large tree might overfit the data, while a small tree might not capture the important structure.
Tree size is a tuning parameter governing the model's complexity, and the optimal tree size could be adaptively chosen from the data.

\subsubsection{Random Forests}
\label{ssub:random_forests}
A random forest~\cite{breiman2001random} is a meta estimator that fits a number of regression trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over fit.
The randomness comes from the process of generating a collection of regression trees, where each tree is trained on a randomly selected subset of training data and a randomly subset of input variables.
Although each individual regression tree may suffer from bias and overfitting, the randomness ensure that the entire forest could capture the entire variance of the feature distribution.
Another advantage of random forests is that it runs efficiently on large data bases, since the algorithm is highly parallelable.

\subsubsection{AdaBoost}
\label{ssub:adaboost}
AdaBoost~\cite{collins2002} is short for Adaptive Boosting.
%, is a meta-estimator that begins by fitting a regressor on original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediciton.
%As such, subsequent regressors focus more on difficult cases.
%AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instance misclassified by previous classifiers.
%The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.
The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. 
The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. 
The data modifications at each so-called boosting iteration consist of applying weights $w_1, w_2, \ldots w_N$ to each of the training samples. 
Initially, those weights are all set to $w_i = \frac{1}{N}$, so that the first step simply trains a weak learner on the original data. 
For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the re-weighted data. 
At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. 
As iterations proceed, examples that are difficult to predict receive ever-increasing influence. 
Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence

\subsubsection{Gradient Boosted Regression Trees}
\label{ssub:gradient_boost}
Gradient boosted regression trees (GBRT)~\cite{friedman2001} is a generalization of boosting to arbitrary differentiable loss functions.
GBRT has natural handling of data of mixed type and is robust to outliers in output space via robust loss functions.
GBRT considers additive models of the following form:
\begin{equation}
	F(x) = \sum\limits_{m=1}^{M} \gamma_m h_m(x)
\end{equation}
where $h_m(x)$ are the basis functions which are usually called weak learners in the context of boosting.
Gradient tree boosting uses regression trees of fixed sizes as weak learners.

\subsection{Nearest Neighbors Regression}
\label{sub:nearest_neighbor}
Except all of the supervised learning approaches we have introduced above, nearest neighbors regression~\cite{hardle1990} is a intuitive but powerful data-driven approach that works well when abundant training data is available.
Essentially, given input vector $X$, we find $k$ nearest vectors from the training set, which are defined as top $k$ vectors with smallest $l_2$ distance to input vector $X$.
Then the predicted value is interpolated from the $k$ nearest neighbor's ground truth value. 
The mathematical formulation is given below:
\begin{equation}
	\hat{Y} = \frac{1}{k} \sum \limits_{x_i \in N_k(x)} y_i
\end{equation}
where $N_k(x)$ is the neighborhood of $x$ defined by the $k$ closest points $x_i$ in the training set.
